{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16. Natural Language Processing with RNNs and Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1fUJVfXmMum4pCFW+Q+QD"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ],
      "metadata": {
        "id": "lluO0-MKIeT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a famous [2015 blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) titled “The Unreasonable Effectiveness of Recurrent Neural Networks,” Andrej Karpathy showed how to train an RNN to predict the next character in a sentence\n",
        "\n",
        "This **Char-RNN** can then be used to generate novel text, one character at a time"
      ],
      "metadata": {
        "id": "JfPF18pCIg-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Training Dataset"
      ],
      "metadata": {
        "id": "rz_zpqHTJCnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let’s download all of Shakespeare’s work, using Keras’s handy get_file() function and downloading the data from Andrej Karpathy’ [Char-RNN project](https://github.com/karpathy/char-rnn):"
      ],
      "metadata": {
        "id": "XlIC-lcqJJJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "sX6rM0CXJo8q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "B5TpAbqIHx2O"
      },
      "outputs": [],
      "source": [
        "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we must encode every character as an integer. One option is to create a custom preprocessing layer, as we did in Chapter 13\n",
        "\n",
        "But in this case, it will be simpler to use Keras’ Tokenizer class. First we need to fit a **tokenizer** to the text: **it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters** (it does not start at 0, so we can use that value for masking, as we will see later in this chapter):"
      ],
      "metadata": {
        "id": "528552bvJ1q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([shakespeare_text])"
      ],
      "metadata": {
        "id": "lNPR-xC2KWTK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set **char_level=True** to get **character-level encoding rather than the default word-level encoding**. Note that this **tokenizer converts the text to lowercase by default** (but you can set lower=False if you do not want that)\n",
        "\n",
        "Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:"
      ],
      "metadata": {
        "id": "4OWS860MKqEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_sequences([\"First\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj0hOHSmKpnt",
        "outputId": "2a515cee-7f08-46c8-e222-97272d60c661"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PpVqeHfLgP1",
        "outputId": "754dfcdc-f7ec-4cb2-a4d3-7bdd4aec4081"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i r s t']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "max_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Da8bub2Lo41",
        "outputId": "57e0cf38-a212-4151-8008-be32ecc64ece"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = tokenizer.document_count # total number of characters\n",
        "dataset_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asqyxRV5LwfL",
        "outputId": "9481ef1c-be47-4bcb-ceb6-f35b16f63a07"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
      ],
      "metadata": {
        "id": "S7U2pVYz_-dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "metadata": {
        "id": "jf1N5p_S_4Aq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we continue, we need to split the dataset into a training set, a validation set, and a test set. We **can’t just shuffle all the characters in the text,** so how do you split a sequential dataset?"
      ],
      "metadata": {
        "id": "WilnAQNbAQC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Split a Sequential Dataset"
      ],
      "metadata": {
        "id": "Saml8x0rAf0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, we can take the first 90% of the text for the training set, then the next 5% for the validation set, and the final 5% for the test set\n",
        "\n",
        "It would also be a good idea to **leave a gap between these sets** to **avoid** the risk of a **paragraph overlapping** over two sets\n",
        "\n",
        "When dealing with time series, you would in **general split across time,:** for example, you might take the years **2000 to 2012 for the training set, the years 2013 to 2015 for the validation set, and the years 2016 to 2018 for the test set.** However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on\n",
        "\n",
        "So, it is often safer to split across time—but this implicitly **assumes that the patterns the RNN can learn in the past** (in the training set) **will still exist in the future.** In other words, we assume that the time series is **stationary**\n",
        "\n",
        "For many time series this assumption is reasonable **(e.g., chemical reactions should be fine, since the laws of chemistry don’t change every day),** but f**or many others it is not (e.g., financial markets are notoriously not stationary since patterns disappear as soon as traders spot them and start exploiting them)**\n",
        "\n",
        "To **make sure** the time series is indeed sufficiently **stationary,** you can **plot the model’s errors on the validation set across time: if the model performs much better on the first part of the validation set than on the last part, then the time series may not be stationary enough,** and you might be **better off training the model on a shorter time span.**\n",
        "\n",
        "Now back to Shakespeare! Let’s take the first 90% of the text for the training set (keeping the rest for the validation set and the test set), and create a tf.data.Dataset that will return each character one by one from this set:"
      ],
      "metadata": {
        "id": "L7FYdN1VA1rU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "metadata": {
        "id": "eUMwdiLbAkiS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chopping the Sequential Dataset into Multiple Windows"
      ],
      "metadata": {
        "id": "mYBvevngDlDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set now consists of a single sequence of **over a million characters,** so we **can’t just train the neural network directly** on it: **the RNN would be equivalent to a deep net with over a million layers,** and we would have a single (very long) instance to train it\n",
        "\n",
        "Instead, we will use the dataset’s **window() method** to **convert** this long sequence of characters **into many smaller windows of text**\n",
        "\n",
        "**Every instance** in the dataset will be a **fairly short substring of the whole text,** and the **RNN will be unrolled only over the length of these substrings.** This is called **truncated backpropagation through time**\n",
        "\n",
        "Let's call the window() method to create a dataset of short text windows:"
      ],
      "metadata": {
        "id": "nbjhOncoDpsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
        "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "6iMrvFwHDsdF"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By **default,** the **window()** method **creates nonoverlapping windows,** but **to get the largest possible training set we use shift=1** so the **first window contains characters 0 to 100. the second contains characters 1 to 101,** and so on\n",
        "\n",
        "To **ensure that all windows are exactly 101 characters** long (which will allow us to create batches without having to do any padding), we set **drop_remainder=True **\n",
        "\n"
      ],
      "metadata": {
        "id": "Ciy0FcEWFvMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "A2hjvmA7FWxL"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}